---
title: "Pause for thought - data analysis"
author: "Zhang Chen"
date: "`r format(Sys.time(), '%d %B, %Y')`"
format: 
  html:
    code-fold: true
editor: visual
execute: 
  warning: false
  error: false
toc: true
toc-depth: 3
server: shiny
---

## Load libraries and data

```{r}
# load libraries
library(MASS)
library(Rmisc)
library(afex)
library(tidyverse)
library(patchwork)

library(extraDistr)
library(loo)
library(bridgesampling)
library(brms)
library(cmdstanr)
library(bayesplot)
library(bayestestR)
library(sjPlot)

# parallelize the chains using all the cores
options(mc.cores = parallel::detectCores())

# set the theme for all ggplot2 figures
theme_set(theme_bw() +
            theme(legend.position = "top",
                  legend.direction = "horizontal"))

# create a folder for saving brms fits
if(!dir.exists("brms-fits")){
  dir.create("brms-fits")
}
```

```{r}
# load the main data files
df_main <- 
  list.files("../../../data/raw/", pattern = "main", full.names = TRUE) %>%
  map_dfr(~read_csv(.x, col_types = cols(.default = col_character())))
```

## Participants

```{r}
# demographics
df_demo <- df_main %>%
  group_by(subject_ID) %>%
  sample_n(1) %>%
  filter(subject_ID != "subject_ID") %>%
  ungroup()

# do some data cleaning
df_demo <- df_demo %>%
  mutate(
    age = as.numeric(age),
    nationality = str_to_title(nationality),
    nationality = recode(nationality, "Italy" = "Italian",
                         "Poland" = "Polish", "Austria" = "Austrian",
                         "Hungary" = "Hungarian")
  )

# compute gender and age statistics
n_total <- nrow(df_demo)
n_male <- sum(df_demo$gender == "male")
n_female <- sum(df_demo$gender == "female")

M_age <- round(mean(df_demo$age), 1)
SD_age <- round(sd(df_demo$age), 1)
```

In total, `r n_total` participants (`r n_male` males, `r n_female` females; M~age~ = `r M_age`; SD~age~ = `r SD_age`) took part in the experiment via Prolific.co on Nov 4th, 2022. Another three participants initially signed up for the experiment but then returned. One participant was timed out. No data was recorded for the latter 4 participants. Eligibility criteria included:

1.  Between 18 and 55 years old;

2.  Having an approval rate of at least 85%;

3.  Having English as one of the fluent languages;

4.  Reporting no color blindness.

@fig-nationality shows the distribution of the self-reported nationalities of all participants.

```{r}
#| label: fig-nationality
#| fig-cap: Distribution of self-reported nationality.
#| fig-width: 5
#| fig-height: 6

# nationality distribution
df_demo %>%
  group_by(nationality) %>%
  mutate(n=n()) %>%
  ggplot(aes(x = reorder(nationality, n))) +
  geom_bar() +
  labs(x = "Nationality", y = "Number of Participants") +
  coord_flip()
```

## Data cleaning

```{r}
# count the number of trials recorded for all participants
trials_count <- df_main %>% count(subject_ID)

# unique(trials_count$n)
# trials_count %>% count(n)

# exclude data from the participant (No. 9) who restarted
df_main <- df_main %>%
  filter(!subject_ID %in% c("9", "subject_ID"))

# check trial count again
trials_count <- df_main %>% count(subject_ID)
```

One participant restarted the experiment half way through Block 5. Data from this participant was excluded. Three participants had 107 trials, thus missing one trial. Data from these three participants was retained. All remaining participants had 108 trials recorded - the correct number of trials. This leaves us with 49 participants for further analysis.

```{r}
# clean the main data frame
df_main <- df_main %>%
  # select the experimental blocks
  filter(exp_part == "exp") %>%
  # some variables should be numeric values
  mutate(
    across(c(subject_ID, age, block_number, game1_startRT,
             game1_respRT, game2_respRT), as.numeric),
    across(game2_delay_premature:game2_LP_amount, as.numeric)
  )

# check the data type of all columns before proceeding
# str(df_main)

# check the number of trials again
trials_count <- df_main %>% count(subject_ID)
```

## Quality checks

### Overall

The experimental blocks consist of (1) experimental pairs, where the game 2 pairs are from the Vancouver Gambling task, and (2) catch pairs, in which the high-prob option dominates the low-prob option. First, I compute overall how often participants picked the high-prob option in the two types of trials (@fig-high-prob-choice). For the catch trials, participants are supposed to always pick the high-probability option.

```{r}
#| label: fig-high-prob-choice
#| fig-cap: The proportion of choosing the high-probability option on the catch trials (top) and the experimental trials (bottom).
#| fig-width: 7
#| fig-height: 5

high_prob_choices <- df_main %>%
  group_by(subject_ID, trial_type) %>%
  summarize(high_prob = mean(game2_choice == "HP") * 100)

# plot the high-prob choices
ggplot(high_prob_choices, aes(subject_ID, high_prob)) +
  geom_line(aes(group = 1), color = "gray") +
  geom_point() +
  labs(x = "Participant", y = "Proportion of high-probability choices") +
  facet_wrap(~trial_type, ncol = 1)
```

In half of the experimental pairs, the high-probability option has higher expected values; in the remaining half, the low-probability option has higher expected values. Next I compute and plot the probability of picking the option with a higher expected value (@fig-high-ev-choice).

```{r}
#| label: fig-high-ev-choice
#| fig-cap: The proportion of choosing the high expected value option.
#| fig-width: 7
#| fig-height: 5

df_main <- df_main %>%
  mutate(
    # compute the expected values of each option
    HP_EV = game2_HP_amount * game2_HP_prob,
    LP_EV = game2_LP_amount * game2_LP_prob,
    # determine which option has a higher expected value
    high_EV_option = ifelse(HP_EV > LP_EV, "HP", "LP"),
    # determine whether participants picked the high EV option
    choose_high_EV = ifelse(game2_choice == high_EV_option, "yes", "no")
  )

# compute the proportion of choosing the high EV option
df_main %>%
  filter(trial_type == "exp") %>%
  group_by(subject_ID, high_EV_option) %>%
  summarize(choose_high_EV = mean(choose_high_EV == "yes") * 100) %>%
  ggplot(aes(subject_ID, choose_high_EV, color = high_EV_option)) +
  geom_line(aes(group = high_EV_option)) +
  geom_point() +
  labs(x = "Participant", y = "The proportion of choosing high EV options",
       color = "Which option has a higher EV?") 
```

When the high-probability option has a higher expected value, participants chose the high-probability option for most of the time. For many participants, this value is close to 100%, leaving little room for previous wins or losses to exert any influence. When the low-probability has a higher expected value, the responses are more variable. Overall, participants were more reluctant to choose the low-probability option. Some participants seemed to have adopt the strategy of choosing the high-probability option for most of the time, even though it entails a lower expected value. There is also more variation across participants. These observations broadly match those in the healthy group in Limbrick-Oldfield et al. (2020).

[**Note to self**]{.underline}: Prior wins and losses may have stronger influences on current choices, when the low-probability option has a higher expected value? This would require analyzing the two situations (i.e. the high-probability option or the low-probability has a higher expected value) separately, which does not seem to be done by Limbrick-Oldfield et al. (2020).

Next I also plot the median choice reaction time for the catch trials and the experimental trials separately (@fig-medianRT).

```{r}
#| label: fig-medianRT
#| fig-cap: The median choice reaction times on the catch trials (top) and the experimental trials (bottom).
#| fig-width: 7
#| fig-height: 5

median_choiceRTs <- df_main %>%
  group_by(subject_ID, trial_type) %>%
  summarize(choiceRT = median(game2_respRT))

ggplot(median_choiceRTs, aes(subject_ID, choiceRT)) +
  geom_line(aes(group = 1), color = "gray") +
  geom_point() +
  labs(x = "Participant", y = "Median choice RT in game 2 (ms)") +
  facet_wrap(~trial_type, ncol = 1)
```

### Per block

Next, I plot the above two figures again, but this time separately for the different blocks (@fig-high-prob-choice-block and @fig-medianRT-block).

```{r}
#| label: fig-high-prob-choice-block
#| fig-cap: The proportion of choosing the high-probability option on the catch trials (top) and the experimental trials (bottom), for each block seprately.
#| fig-width: 7
#| fig-height: 5

df_main %>%
  group_by(subject_ID, trial_type, block_number) %>%
  summarize(high_prob = mean(game2_choice == "HP") * 100) %>%
  mutate(block_number = as.factor(block_number)) %>%
  ggplot(aes(subject_ID, high_prob, color = block_number)) +
  geom_line(aes(group = block_number)) +
  geom_point() +
  labs(x = "Participant", y = "Proportion of high-probability choices",
       color = "Block Number") +
  facet_wrap(~trial_type, ncol = 1)
```

```{r}
#| label: fig-medianRT-block
#| fig-cap: The median choice reaction times on the catch trials (top) and the experimental trials (bottom), for each block separately.
#| fig-width: 7
#| fig-height: 5

df_main %>%
  group_by(subject_ID, trial_type, block_number) %>%
  summarize(choiceRT = median(game2_respRT)) %>%
  mutate(block_number = as.factor(block_number)) %>%
  ggplot(aes(subject_ID, choiceRT, color = block_number)) +
  geom_line(aes(group = block_number)) +
  geom_point() +
  labs(x = "Participant", y = "Median choice RT in game 2 (ms)",
       color = "Block Number") +
  facet_wrap(~trial_type, ncol = 1)
```

### Exclusion criteria

For now, I adopt the exclusion criteria that participants need to choose the high-probability option on at least 80% of the catch trials in order to be included in the analysis. Data from three participants is excluded, leaving 46 participants in further analysis. Note that this does not mean the remaining participants were necessarily paying full attention to the task. For instance, a few participants seemed to have adopted the strategy of always picking the high-probability option. They probably did not pay a lot of attention to the two options presented on each trial.

[**Note to self**]{.underline}: In future studies, it would be useful to include catch trials on which the low-probability option is the better choice. For instance, when presented with a 60% probability of winning 60 cents, and a 70% probability of winning 10 cents, most people would presumably agree that the 60% option is better. Including such trials would allow us to identify participants who always go for the high-probability option without processing the relevant information.

```{r}

# participants to be excluded
subject_exclude <- high_prob_choices %>%
  filter(
    trial_type == "catch",
    high_prob < 80
  ) %>%
  .$subject_ID

df_exp <- df_main %>%
  filter(
    !subject_ID %in% subject_exclude,
    # experimental trials only
    trial_type == "exp"
  )

# check the number of remaining participants is correct
# n_distinct(df_exp$subject_ID)

# an alternative way of cleaning data
# since the experimental trials were divided into 4 blocks,
# next I only include blocks where participants chose the HP option
# on at least 4 catch trials in each block.

# # count how often people picked the HP option in each block
# df_catch_choices <- df_main %>%
#   filter(trial_type == "catch") %>%
#   group_by(subject_ID, block_number) %>%
#   summarize(catch_count = sum(game2_choice == "HP"))
# 
# # add the count of HP choices to the main data frame
# df_main <- df_main %>%
#   full_join(df_catch_choices, by = c("subject_ID", "block_number"))
# 
# # select blocks where people chose the HP option on at least 4 catch trials
# df_exp <- df_main %>%
#   filter(
#     catch_count >= 4,
#     # experimental trials only
#     trial_type == "exp"
#     )
```

## Game 2 start RTs

Next I first examine how quickly participants started game 2 after a win vs. after a loss, and also as a function of whether there was a delay or not. I use the same data preparation and analysis strategy as in our previous work.

```{r}

# trial number before any exclusion
n_before <- nrow(df_exp)

# exclude trials in which the start RTs were above 5000 milliseconds
df_after <- df_exp %>%
  filter(game2_startRT <= 5000)

# trial number after exclusion, and the proportion of excluded trials
n_after <- nrow(df_after)
exclude_prop <- round((n_before - n_after)/n_before * 100, 2)

# check the number of remaining trials in each cell for each participant
trials_count <- df_after %>%
  count(subject_ID, game1_outcome, delay)
```

Trials in which the start RT of game 2 was above 5000 milliseconds were excluded (`r exclude_prop`% of all trials). For the remaining participants, each cell in the 2 (game 1 outcome, win vs. loss) by 2 (delay, yes vs. no) contains at least `r min(trials_count$n)` trials, which seems reasonable. For each participant, I then compute the mean start RT (for game 2) after a win or a loss in game 1, depending on whether there was a delay or not.

```{r}
#| label: fig-game2-startRT
#| fig-cap: Start RT for game 2 as a function of game 1 outcome and delay. Error bars stand for 95% within-subject confidence intervals.
#| fig-width: 6
#| fig-height: 5

# compute the mean start RT in each cell for each participant
df_startRTs <- df_after %>%
  group_by(subject_ID, game1_outcome, delay) %>%
  summarize(startRT = mean(game2_startRT))

# plot the results
df_startRTs %>%
  summarySEwithin(measurevar = "startRT",
                  withinvars = c("game1_outcome", "delay"),
                  idvar = "subject_ID") %>%
  ggplot(aes(delay, startRT, color = game1_outcome)) +
  geom_point(position = position_dodge(width = 0.5)) +
  geom_errorbar(aes(ymin = startRT - ci, ymax = startRT +ci),
                position = position_dodge(width = 0.5), width = 0.3) +
  labs(x = "Delay between two games", 
       y = "Reaction time of starting game 2 (milliseconds)",
       color = "Game 1 outcome")
```

```{r}
# run a repeated-measures ANOVA
anova_startRT <- 
  aov_ez(id = "subject_ID",
         dv = "startRT",
         data = df_startRTs,
         within = c("game1_outcome", "delay"))

nice(anova_startRT)
```

Wins and losses in game 1 did not significantly influence how quickly participants started game 2 (if anything, participants seemed to start game 2 more ***slowly*** after a loss than after a win). When there was a delay, participants tended to start game 2 a bit more slowly, but this effect was also not statistically significant.

[**Note to self**]{.underline}: Currently, game 1 is always the same. Participants may have quickly learned that their choice actually does not matter - the result is always randomly determined. Their engagement level may have quickly decreased, which may explain why I did not observe a post-loss speeding effect here. For future studies, it may be useful to introduce some variation. For instance, the wheel may be modified, with some containing a larger blue area, others a larger yellow area. This can become another manipulation check - participants should always choose the dominant color in order to maximize their chance of winning.

## Game 2 choices

### Aggregated data

I first compute the aggregated proportion of choosing the high-probability option in each cell, for each participant. For this, I include all trials (i.e., even though with start RTs \> 5000 milliseconds). The reason for making this analysis decision is because if certain trials were excluded, the choice pairs included in each cell may not be exactly comparable any more.

```{r}
#| label: fig-game2-HP-choice
#| fig-cap: Proportion of high-probability choices in game 2 as a function of game 1 outcome and delay. Error bars stand for 95% within-subject confidence intervals.
#| fig-width: 6
#| fig-height: 5

df_HP_choices <- df_exp %>%
  group_by(subject_ID, game1_outcome, delay) %>%
  summarize(HP_choice = mean(game2_choice == "HP") * 100)

# plot the results
df_HP_choices %>%
  summarySEwithin(measurevar = "HP_choice",
                  withinvars = c("game1_outcome", "delay"),
                  idvar = "subject_ID") %>%
  ggplot(aes(delay, HP_choice, color = game1_outcome)) +
  geom_point(position = position_dodge(width = 0.5)) +
  geom_errorbar(aes(ymin = HP_choice - ci, ymax = HP_choice +ci),
                position = position_dodge(width = 0.5), width = 0.3) +
  labs(x = "Delay between two games", 
       y = "Proportion of high-probability choices",
       color = "Game 1 outcome")
```

As noted above, when the high-probability option was the option with a higher expected value, participants predominantly chose the high-probability option with little variation. I therefore plot the data again, this time separately for whether the low-probability or the high-probability was the one with a higher expected value.

```{r}
#| label: fig-game2-HP-choice-separate
#| fig-cap: Proportion of high-probability choices in game 2 as a function of game 1 outcome and delay, separately for when the high-probability option (left) or the low-probability option (right) has a higher expected value. Error bars stand for 95% within-subject confidence intervals.
#| fig-width: 8
#| fig-height: 5

df_HP_choices_separate <- df_exp %>%
  group_by(subject_ID, game1_outcome, delay, high_EV_option) %>%
  summarize(HP_choice = mean(game2_choice == "HP") * 100)

# plot the results
df_HP_choices_separate %>%
  summarySEwithin(measurevar = "HP_choice",
                  withinvars = c("game1_outcome", "delay", "high_EV_option"),
                  idvar = "subject_ID") %>%
  ggplot(aes(delay, HP_choice, color = game1_outcome)) +
  geom_point(position = position_dodge(width = 0.5)) +
  geom_errorbar(aes(ymin = HP_choice - ci, ymax = HP_choice +ci),
                position = position_dodge(width = 0.5), width = 0.3) +
  labs(x = "Delay between two games", 
       y = "Proportion of high-probability choices",
       color = "Game 1 outcome") +
  facet_wrap(~high_EV_option, ncol = 2)
```

Wins and losses seem to have very little influence on choices. However, a potentially interesting observation (if reliable) is that after a delay, participants seem to become more sensitive to expected values in their choices. When the HP option has a higher expected value, they chose the HP option more often after a delay; when the LP option has a higher expected value, they chose the LP option more often after a delay.

### Multilevel analyses

In this analysis, I use the EV ratio between two options, the outcome of game 1 (loss = 0.5, win = -0.5) and whether there was a delay or not (yes = 0.5, no = -0.5) as predictors to predict whether participants chose the HP option or not on each trial. All interactions are also included, and I use the maximum random structure by including both random intercept and all random slopes per participant.

```{r}
# recoding the predictors and the outcome variable
df_exp <- df_exp %>%
  mutate(
    # compute the EV ratio on each trial
    EV_ratio = (HP_EV - LP_EV)/(HP_EV + LP_EV) * 2,
    # use effect-coding for both categorical predictors
    game1_outcome_num = ifelse(game1_outcome == "loss", 0.5, -0.5),
    delay_num = ifelse(delay == "yes", 0.5, -0.5),
    # the outcome variable: choose HP = 1
    game2_choose_HP = ifelse(game2_choice == "HP", 1, 0)
  )

# fit a brms model - go for the maximum random structure
fit_HP_choice <- brm(
  game2_choose_HP ~ EV_ratio * game1_outcome_num * delay_num +
    (EV_ratio * game1_outcome_num * delay_num|subject_ID),
  family = bernoulli(link = "logit"),
  prior = 
    c(
      prior(normal(0, 2), class = Intercept),
      prior(normal(0, 1), class = b),
      prior(normal(0, 1), class = sd),
      prior(lkj(2), class = cor)
    ),
  data = df_exp,
  cores = 4,
  iter = 4000,
  warmup = 2000,
  seed = 1234,
  file = "brms-fits/fit_HP_choice",
  backend = "cmdstanr"
)
```

Some general posterior predictive checks. While the model seems to capture the effects of game 1 outcome and delay relatively well, there is a large deviation between the observed and predicted outcome when it comes to EV ratio.

```{r}
#| label: fig-ppc-general
#| fig-cap: General posterior predictive checks.
#| fig-subcap: 
#| - "As a function of delay"
#| - "As a function of game 1 outcome"
#| - "As a function of EV ratio"
#| layout: [[1, 1], [1]]
#| fig-width: 8
#| fig-height: 8


# model diagnostics
# check the trace plot, looks good.
# plot(fit_HP_choice)

# posterior predictive checks
yrep <- posterior_predict(fit_HP_choice)

ppc_bars_grouped(df_exp$game2_choose_HP, yrep, 
                 df_exp$delay, prob = 0.95, freq = FALSE)

ppc_bars_grouped(df_exp$game2_choose_HP, yrep, 
                 df_exp$game1_outcome, prob = 0.95, freq = FALSE)

ppc_bars_grouped(df_exp$game2_choose_HP, yrep, 
                 df_exp$EV_ratio, prob = 0.95, freq = FALSE)

# save the raw data frame and yrep as rds files.
# This is needed for the Shiny code below
saveRDS(df_exp, "brms-fits/data.rds")
saveRDS(yrep, "brms-fits/predicted.rds")
```

Next I make a custom posterior predictive check for each participant separately, focusing on the effect of EV ratio. Note that this code uses Shiny, and will only run when the Quarto document is executed within RStudio (click on the Run Document button).

```{r}
#| context: setup
#| include: false

# load libraries
library(tidyverse)
library(wrMisc)
library(bayestestR)
library(shiny)

# load data
observed <- readRDS("brms-fits/data.rds")
predicted <- readRDS("brms-fits/predicted.rds")

# function to compute the median, and boundaries of 95% HDI
compute_summary <- function(x){
  median <- median(x) * 100
  upperCI <- hdi(x)$CI_high * 100
  lowerCI <- hdi(x)$CI_low * 100
  return(c(median = median, upperCI = upperCI, lowerCI = lowerCI))
}
```

```{r}
selectInput(
  inputId = "subject_selected",
  label = "Select a participant number",
  choices = unique(df_exp$subject_ID),
  selected = unique(df_exp$subject_ID)[1],
  multiple = FALSE,
  selectize = TRUE,
  width = NULL,
  size = NULL
)

plotOutput("ppcPlot")
```

```{r}
#| context: server

output$ppcPlot <- renderPlot({
  # select the observed and predicted data for a particular subject
  subject_selected <- input$subject_selected

  df_exp_one_pp <- observed %>% filter(subject_ID == subject_selected)
  yrep_one_pp <- predicted[, observed$subject_ID == subject_selected]

  # compute the mean proportion of choosing the HP option,
  # given each EV ratio level, for each simulation separately
  yrep_prop <- rowGrpMeans(yrep_one_pp, grp = df_exp_one_pp$EV_ratio)

  # turn the data into a tibble
  yrep_prop <- as_tibble(yrep_prop)

  # compute the median and 95% HDI for each EV ratio level
  yrep_summary <- sapply(yrep_prop, compute_summary)

  # do some data formatting
  yrep_summary <- as_tibble(t(yrep_summary), rownames = NA) %>%
    rownames_to_column(var = "EV_ratio") %>%
    mutate(EV_ratio = as.numeric(EV_ratio)) %>%
    arrange(EV_ratio)

  # compute the observed proportion of HP choices for each EV ratio
  y_summary <- df_exp_one_pp %>%
    group_by(EV_ratio) %>%
    summarize(y = mean(game2_choice == "HP") * 100) %>%
    arrange(EV_ratio) %>%
    select(-EV_ratio)

  # combine the observed and simulated data
  summary <- yrep_summary %>%
    bind_cols(y_summary)

  # plot the results
  ggplot(summary, aes(EV_ratio, y)) +
    # the observed proportion of HP choices as gray dots
    geom_point(size = 4, color = "gray") +
    # the median predicted proportion of HP choices as blue horizontal lines
    geom_point(aes(EV_ratio, median), 
               color = "deepskyblue", shape = "-", size = 10) +
    # the 95% HDI of the predicted HP choices as blue vertical lines 
    geom_linerange(aes(ymin = lowerCI, ymax = upperCI), 
                   color = "deepskyblue") +
    ylim(0, 100) +
    labs(x = "EV Ratio", y = "Porportion of HP choices (%)",
         title = paste0("Participant ", subject_selected)) +
    theme_bw()
})
```

```{r}
#| label: fig-effect
#| fig-cap: Posterior distributions of the regression coefficients.
#| fig-width: 10
#| fig-height: 6

# plot the posterior distributions for variables of interest
var_selected <- variables(fit_HP_choice)[2:8]

mcmc_areas(fit_HP_choice, pars = var_selected)
```

```{r}
tab_model(fit_HP_choice)
```

After a delay, participants became more sensitive to the EV ratios in their choices. However, the posterior predictive checks show that the model does not predict the effects of EV ratio very well. When the EV ratio is negative (i.e., when the LP option has a higher EV), the predicted choices show large deviations from the observed choices.

[**Note to self**]{.underline}: May be informative to fit the Cumulative Prospect Theory model to the current data (and that of Limbrick-Oldfield et al.) to see if the model fit can be improved. It may further tell us which parameter is influenced by including the delay (and in Limbrick-Oldfield et al., between the two groups).

## Game 2 choice RT

### Aggregated data

Next I examine whether the increased sensitivity towards the EV ratio after a delay may be explained by participants making decisions more slowly in the delay condition.

```{r}
#| label: fig-game2-choiceRT
#| fig-cap: Histogram of all game 2 choice reaction times.
#| fig-width: 5
#| fig-height: 4

# plot a distribution of all game 2 choice RTs
ggplot(df_exp, aes(game2_respRT)) +
  geom_histogram(bins = 40) +
  geom_vline(xintercept = 5000, linetype = "dashed") +
  labs(x = "Game 2 choice reaction time (milliseconds)")
```

To reduce the influence of a few long reaction times, I exclude the trials where the choiceRT was above 5000 milliseconds.

```{r}
#| label: fig-game2-choiceRT-per-cell
#| fig-cap: Choice RT in game 2 as a function of game 1 outcome and delay. Error bars stand for 95% within-subject confidence intervals.
#| fig-width: 6
#| fig-height: 5

df_choiceRT <- df_exp %>% filter(game2_respRT <= 5000)

df_choiceRT_summary <- df_choiceRT %>%
  group_by(subject_ID, game1_outcome, delay) %>%
  summarize(choiceRT = mean(game2_respRT))

# plot the choice RTs
df_choiceRT_summary %>%
  summarySEwithin(measurevar = "choiceRT",
                  withinvars = c("game1_outcome", "delay"),
                  idvar = "subject_ID") %>%
  ggplot(aes(delay, choiceRT, color = game1_outcome)) +
  geom_point(position = position_dodge(width = 0.5)) +
  geom_errorbar(aes(ymin = choiceRT - ci, ymax = choiceRT +ci),
                position = position_dodge(width = 0.5), width = 0.3) +
  labs(x = "Delay between two games", 
       y = "Game 2 choice reaction time (milliseconds)",
       color = "Game 1 outcome")
```

```{r}
# run a repeated-measures ANOVA
anova_choiceRT <- 
  aov_ez(id = "subject_ID",
         dv = "choiceRT",
         data = df_choiceRT_summary,
         within = c("game1_outcome", "delay"))

nice(anova_choiceRT)
```

### Multilevel analyses

I use log-transformed choice RT as the dependent variable. In total I tried four brms models, with either a Student's t or a Gaussian function as the likelihood, and whether the EV ratio is included as a predictor. Both game 1 outcome, whether there was a pause or not and their interaction are always included as predictors.

```{r}

# log-transform the choice RT
df_choiceRT <- df_choiceRT %>%
  mutate(log_choiceRT = log(game2_respRT))

# fit a brms model, with student's t as the likelihood function
fit_choiceRT_student <- brm(
  log_choiceRT ~  game1_outcome_num * delay_num +
    (game1_outcome_num * delay_num|subject_ID),
  family = student(),
  prior = 
    c(
      prior(normal(6.5, 1.5), class = Intercept),
      prior(normal(0, 1), class = b),
      prior(normal(0, 1), class = sd),
      prior(normal(0, 1), class = sigma),
      prior(gamma(2, 0.1), class = nu),
      prior(lkj(2), class = cor)
    ),
  data = df_choiceRT,
  cores = 4,
  iter = 4000,
  warmup = 2000,
  seed = 1234,
  file = "brms-fits/fit_choiceRT_student",
  backend = "cmdstanr"
  )

# try one with a normal likelihood
fit_choiceRT_normal <- brm(
  log_choiceRT ~  game1_outcome_num * delay_num +
    (game1_outcome_num * delay_num|subject_ID),
  family = gaussian(),
  prior = 
    c(
      prior(normal(6.5, 1.5), class = Intercept),
      prior(normal(0, 1), class = b),
      prior(normal(0, 1), class = sd),
      prior(normal(0, 1), class = sigma),
      prior(lkj(2), class = cor)
    ),
  data = df_choiceRT,
  cores = 4,
  iter = 4000,
  warmup = 2000,
  seed = 1234,
  file = "brms-fits/fit_choiceRT_normal",
  backend = "cmdstanr"
  )

# what if I add EV ratio? With student's t as likelihood
fit_choiceRT_withEV_student <- brm(
  log_choiceRT ~  EV_ratio * game1_outcome_num * delay_num +
    (EV_ratio* game1_outcome_num * delay_num|subject_ID),
  family = student(),
  prior = 
    c(
      prior(normal(6.5, 1.5), class = Intercept),
      prior(normal(0, 1), class = b),
      prior(normal(0, 1), class = sd),
      prior(normal(0, 1), class = sigma),
      prior(gamma(2, 0.1), class = nu),
      prior(lkj(2), class = cor)
    ),
  data = df_choiceRT,
  cores = 4,
  iter = 4000,
  warmup = 2000,
  seed = 1234,
  file = "brms-fits/fit_choiceRT_withEV_student",
  backend = "cmdstanr"
  )

# with EV ratio as a predictor, gaussian likelihood
fit_choiceRT_withEV_normal <- brm(
  log_choiceRT ~  EV_ratio * game1_outcome_num * delay_num +
    (EV_ratio* game1_outcome_num * delay_num|subject_ID),
  family = gaussian(),
  prior = 
    c(
      prior(normal(6.5, 1.5), class = Intercept),
      prior(normal(0, 1), class = b),
      prior(normal(0, 1), class = sd),
      prior(normal(0, 1), class = sigma),
      prior(lkj(2), class = cor)
    ),
  data = df_choiceRT,
  cores = 4,
  iter = 4000,
  warmup = 2000,
  seed = 1234,
  file = "brms-fits/fit_choiceRT_withEV_normal",
  backend = "cmdstanr"
  )

```

```{r}
# posterior predictive checks, 
# the results from the two models are largely in line 
pp_check(fit_choiceRT_student, ndraws = 30)
# pp_check(fit_choiceRT_normal, ndraws = 30)

pp_check(fit_choiceRT_withEV_student, ndraws = 30)
# pp_check(fit_choiceRT_withEV_normal, ndraws = 30)
```

Model that includes the EV ratio as a predictor, and uses Student's t as the likelihood function.

```{r}
tab_model(fit_choiceRT_withEV_student)
```

A few findings are worth noting, and largely consistent across the models:

1.  Participants overall chose more quickly after a loss than after a win.

2.  Inserting a pause did not reliably influence how quickly they chose.

3.  Larger EV ratios were associated with faster choices. Participants might find a choice easier to make if the HP option has a higher EV.

4.  Delay interacted with EV ratio, but only in the model assuming a Student's t likelihood. (In the Gaussian model, the effect was in the same direction, but the 95% CI included 0.) Thus, choice RTs were influenced by the EV ratio more after a pause.

The interaction effect are broadly in line with the effect observed above for choices. After a pause, both participants' choices and their choice RTs are more influenced by the EV ratios. Their choices overall do not become slower though after a pause.

## Premature responses

When there is a pause between game 1 and game 2, the program also registers whether participants make any responses during the 4-second pause. First, I examine how frequently people made such premature responses, and the number of premature responses during one pause.

```{r}
#| label: fig-premature
#| fig-cap: Premature responses during the delay between game 1 and game 2.
#| fig-subcap: 
#| - "Proprotion of delay trials on which premature responses occurred"
#| - "Histogram of the number of premature responses during a delay"
#| layout: [[1], [1]]
#| fig-width: 5
#| fig-height: 4

# select trials where there was a pause
df_premature <- df_exp %>%
  filter(delay == "yes")

# plot the proportion of trials on which people made premature responses
df_premature %>%
  group_by(subject_ID) %>%
  summarize(premature_prop = mean(game2_delay_premature > 0) * 100) %>%
  ggplot(aes(subject_ID, premature_prop)) +
  geom_point() +
  labs(x = "Participant ID", 
       y = "Proportion of trials with premature responses")

# plot the number of premature responses when they did occur
df_premature %>%
  filter(game2_delay_premature > 0) %>%
  ggplot(aes(game2_delay_premature)) +
  geom_bar() +
  labs(x = "Number of premature responses")
```

@fig-premature shows that participants overall made premature responses frequently (i.e. on many delay trials), but most of the time they only made one premature response during the 4-second pause. This suggests that they probably pressed the space bar quickly after game 1, in an attempt to start game 2, but stopped responding when they realized that they had to wait. Being so frustrated that people started pressing keys multiple times during the delay, actually did not occur very frequently. As such, I do not conduct further analyses on these premature responses.

## Summary

Wins and losses in game 1 do not influence how quickly people started game 2, but how quickly they chose in game 2 (faster choices after losses compared to wins). Wins and losses do not influence people's choices in game 2. There is an effect of pause though. After a pause, their choices become more sensitive to the EV ratio of the two options.
