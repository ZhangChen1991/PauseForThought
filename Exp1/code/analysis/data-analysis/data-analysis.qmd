---
title: "Pause for thought Experiment 1 - data analysis"
author: "Zhang Chen"
date: "`r format(Sys.time(), '%d %B, %Y')`"
format: 
  html:
    code-fold: true
editor: visual
execute: 
  warning: false
  error: false
toc: true
toc-depth: 3
server: shiny
bibliography: references.bib
---

## Load libraries and data

```{r}
# load libraries
library(MASS)
library(Rmisc)
library(afex)
library(tidyverse)
library(patchwork)

library(extraDistr)
library(loo)
library(bridgesampling)
library(brms)
library(cmdstanr)
library(bayesplot)
library(bayestestR)
library(sjPlot)

# parallelize the chains using all the cores
options(mc.cores = parallel::detectCores())

# set the theme for all ggplot2 figures
theme_set(theme_classic() +
            theme(legend.position = "top",
                  legend.direction = "horizontal"))

# create a folder for saving brms fits
if(!dir.exists("brms-fits")){
  dir.create("brms-fits")
}
```

```{r}
# load the main data files
df_main <- 
  list.files("../../../data/raw/", pattern = "main", full.names = TRUE) %>%
  map_dfr(~read_csv(.x, col_types = cols(.default = col_character())))
```

## Participants

```{r}
# demographics
df_demo <- df_main %>%
  group_by(subject_ID) %>%
  sample_n(1) %>%
  filter(subject_ID != "subject_ID") %>%
  ungroup()

# do some data cleaning
df_demo <- df_demo %>%
  mutate(
    age = as.numeric(age),
    nationality = str_to_title(nationality),
    nationality = recode(nationality, "Italy" = "Italian",
                         "Poland" = "Polish", "Austria" = "Austrian",
                         "Hungary" = "Hungarian")
  )

# compute gender and age statistics
n_total <- nrow(df_demo)
n_male <- sum(df_demo$gender == "male")
n_female <- sum(df_demo$gender == "female")

M_age <- round(mean(df_demo$age), 1)
SD_age <- round(sd(df_demo$age), 1)
```

In total, `r n_total` participants (`r n_male` males, `r n_female` females; M~age~ = `r M_age`; SD~age~ = `r SD_age`) took part in the experiment via Prolific.co on Nov 4th, 2022. Another three participants initially signed up for the experiment but then returned. One participant was timed out. No data was recorded for the latter 4 participants. Eligibility criteria included:

1.  Between 18 and 55 years old;

2.  Having an approval rate of at least 85%;

3.  Having English as one of the fluent languages;

4.  Reporting no color blindness.

@fig-nationality shows the distribution of the self-reported nationalities of all participants.

```{r}
#| label: fig-nationality
#| fig-cap: Distribution of self-reported nationality.
#| fig-width: 5
#| fig-height: 6

# nationality distribution
df_demo %>%
  group_by(nationality) %>%
  mutate(n=n()) %>%
  ggplot(aes(x = reorder(nationality, n))) +
  geom_bar() +
  labs(x = "Nationality", y = "Number of Participants") +
  coord_flip()
```

## Data cleaning

```{r}
# count the number of trials recorded for all participants
trials_count <- df_main %>% count(subject_ID)

# unique(trials_count$n)
# trials_count %>% count(n)

# exclude data from the participant (No. 9) who restarted
df_main <- df_main %>%
  filter(!subject_ID %in% c("9", "subject_ID"))

# check trial count again
trials_count <- df_main %>% count(subject_ID)

# check if a certain trial had been recorded multiple times or not
# this has occurred, although infrequently, in previous experiments
unique_acount <- df_main %>%
  mutate(
    unique_trial = paste(subject_ID, exp_part, 
                         block_number, trial_number, sep = "_")
  ) %>%
  count(unique_trial)

# okay, not the case here. Okay to proceed.
```

One participant restarted the experiment half way through Block 5. Data from this participant was excluded. Three participants had 107 trials, thus missing one trial. Data from these three participants were retained. All remaining participants had 108 trials recorded - the correct number of trials. This leaves us with 49 participants for further analysis.

```{r}
# clean the main data frame
df_main <- df_main %>%
  # select the experimental blocks
  filter(exp_part == "exp") %>%
  # some variables should be numeric values
  mutate(
    across(c(subject_ID, age, block_number, game1_startRT,
             game1_respRT, game2_respRT), as.numeric),
    across(game2_delay_premature:game2_LP_amount, as.numeric)
  )

# compute some new variables
df_main <- df_main %>%
  mutate(
    # compute the expected values of each option
    HP_EV = game2_HP_amount * game2_HP_prob,
    LP_EV = game2_LP_amount * game2_LP_prob,
    # determine which option has a higher expected value
    high_EV_option = ifelse(HP_EV > LP_EV, "HP", "LP"),
    # determine whether participants picked the high EV option
    choose_high_EV = ifelse(game2_choice == high_EV_option, "yes", "no"),
    # compute the EV ratio on each trial
    EV_ratio = (HP_EV - LP_EV)/(HP_EV + LP_EV) * 2,
    # use effect-coding for both categorical predictors
    game1_outcome_num = ifelse(game1_outcome == "loss", 0.5, -0.5),
    delay_num = ifelse(delay == "yes", 0.5, -0.5),
    # the outcome variable: choose HP = 1, choose LP = 0
    game2_choose_HP = ifelse(game2_choice == "HP", 1, 0)
  )

# check the data type of all columns before proceeding
# str(df_main)

# check the number of trials again
trials_count <- df_main %>% count(subject_ID)
```

## Quality checks

### Overall

The experimental blocks consist of (1) experimental pairs, where the game 2 pairs are from the Vancouver Gambling task, and (2) catch pairs, in which the high-prob option dominates the low-prob option. First, I compute overall how often participants picked the high-prob option in the two types of trials (@fig-high-prob-choice). For the catch trials, participants are supposed to always pick the high-probability option.

```{r}
#| label: fig-high-prob-choice
#| fig-cap: The proportion of choosing the high-probability option on the catch trials (top) and the experimental trials (bottom).
#| fig-width: 7
#| fig-height: 5

high_prob_choices <- df_main %>%
  group_by(subject_ID, trial_type) %>%
  summarize(
    # the total number of catch and experimental trials
    n_total = n(),
    # the number of times people choose the high-prob option
    n_HP_choice = sum(game2_choice == "HP"),
    # the proportion of choosing the high-prob option
    high_prob = mean(game2_choice == "HP") * 100
    )

# plot the high-prob choices
ggplot(high_prob_choices, aes(subject_ID, high_prob)) +
  geom_line(aes(group = 1), color = "gray") +
  geom_point() +
  labs(x = "Participant", y = "Proportion of high-probability choices") +
  facet_wrap(~trial_type, ncol = 1)
```

In half of the experimental pairs, the high-probability option has higher expected values; in the remaining half, the low-probability option has higher expected values. Next I compute and plot the probability of picking the option with a higher expected value (@fig-high-ev-choice).

```{r}
#| label: fig-high-ev-choice
#| fig-cap: The proportion of choosing the high expected value option.
#| fig-width: 7
#| fig-height: 5

# compute the proportion of choosing the high EV option
df_main %>%
  filter(trial_type == "exp") %>%
  group_by(subject_ID, high_EV_option) %>%
  summarize(choose_high_EV = mean(choose_high_EV == "yes") * 100) %>%
  ggplot(aes(subject_ID, choose_high_EV, color = high_EV_option)) +
  geom_line(aes(group = high_EV_option)) +
  geom_point() +
  labs(x = "Participant", y = "The proportion of choosing high EV options",
       color = "Which option has a higher EV?") 
```

When the high-probability option has a higher expected value, participants chose the high-probability option for most of the time. For many participants, this value is close to 100%, leaving little room for previous wins or losses to exert any influence. When the low-probability has a higher expected value, the responses are more variable. Overall, participants were more reluctant to choose the low-probability option. Some participants seemed to have adopt the strategy of choosing the high-probability option for most of the time, even though it entails a lower expected value. There is also more variation across participants. These observations broadly match those in the healthy group in Limbrick-Oldfield et al. [@limbrick-oldfield2020].

Next I also plot the median choice reaction time for the catch trials and the experimental trials separately (@fig-medianRT).

```{r}
#| label: fig-medianRT
#| fig-cap: The median choice reaction times on the catch trials (top) and the experimental trials (bottom).
#| fig-width: 7
#| fig-height: 5

median_choiceRTs <- df_main %>%
  group_by(subject_ID, trial_type) %>%
  summarize(choiceRT = median(game2_respRT))

ggplot(median_choiceRTs, aes(subject_ID, choiceRT)) +
  geom_line(aes(group = 1), color = "gray") +
  geom_point() +
  labs(x = "Participant", y = "Median choice RT in game 2 (ms)") +
  facet_wrap(~trial_type, ncol = 1)
```

### Per block

Next, I plot the above two figures again, but this time separately for the different blocks (@fig-high-prob-choice-block and @fig-medianRT-block).

```{r}
#| label: fig-high-prob-choice-block
#| fig-cap: The proportion of choosing the high-probability option on the catch trials (top) and the experimental trials (bottom), for each block seprately.
#| fig-width: 7
#| fig-height: 5

df_main %>%
  group_by(subject_ID, trial_type, block_number) %>%
  summarize(high_prob = mean(game2_choice == "HP") * 100) %>%
  mutate(block_number = as.factor(block_number)) %>%
  ggplot(aes(subject_ID, high_prob, color = block_number)) +
  geom_line(aes(group = block_number)) +
  geom_point() +
  labs(x = "Participant", y = "Proportion of high-probability choices",
       color = "Block Number") +
  facet_wrap(~trial_type, ncol = 1)
```

```{r}
#| label: fig-medianRT-block
#| fig-cap: The median choice reaction times on the catch trials (top) and the experimental trials (bottom), for each block separately.
#| fig-width: 7
#| fig-height: 5

df_main %>%
  group_by(subject_ID, trial_type, block_number) %>%
  summarize(choiceRT = median(game2_respRT)) %>%
  mutate(block_number = as.factor(block_number)) %>%
  ggplot(aes(subject_ID, choiceRT, color = block_number)) +
  geom_line(aes(group = block_number)) +
  geom_point() +
  labs(x = "Participant", y = "Median choice RT in game 2 (ms)",
       color = "Block Number") +
  facet_wrap(~trial_type, ncol = 1)
```

### Exclusion criteria

```{r}
# get descriptives of the number of HP choices on catch trials
HP_choice_catch <- high_prob_choices %>%
  filter(trial_type == "catch")

mean_HP_choice_catch <- round(mean(HP_choice_catch$n_HP_choice), 1)
sd_HP_choice_catch <- round(sd(HP_choice_catch$n_HP_choice), 1)
range_HP_choice_catch <- range(HP_choice_catch$n_HP_choice)

# participants to be excluded
subject_exclude <- HP_choice_catch %>%
  filter(
    trial_type == "catch",
    n_HP_choice < 15
  ) %>%
  .$subject_ID

df_exp <- df_main %>%
  filter(
    !subject_ID %in% subject_exclude,
    # experimental trials only
    trial_type == "exp"
  )

# check the number of remaining participants is correct
# n_distinct(df_exp$subject_ID)

# save the cleaned data as an RDS file for other analyses
saveRDS(df_exp, file = "../../../data/processed/df_cleaned.rds")
```

For now, I adopt the exclusion criteria that participants need to choose the high-probability option on at least 15 of the 20 catch trials (or equivalently, 75% of catch trials) in order to be included in the analysis. Data from two participants is excluded, leaving 47 participants in further analysis. Note that this does not mean the remaining participants were necessarily paying full attention to the task. For instance, a few participants seemed to have adopted the strategy of always picking the high-probability option. They might not have payed a lot of attention to the two options on a choice game. In a later section I also carry out a robustness check, in which I vary the cutoff value used for excluding participant.

## Game 2 choices

### Aggregated data

I first compute the aggregated proportion of choosing the high-probability option in each cell, for each participant.

```{r}
#| label: fig-game2-HP-choice
#| fig-cap: Proportion of high-probability choices in game 2 as a function of game 1 outcome and delay. Error bars stand for 95% within-subject confidence intervals.
#| fig-width: 6
#| fig-height: 5

df_HP_choices <- df_exp %>%
  group_by(subject_ID, game1_outcome, delay) %>%
  summarize(HP_choice = mean(game2_choice == "HP") * 100)

# plot the results
df_HP_choices %>%
  summarySEwithin(measurevar = "HP_choice",
                  withinvars = c("game1_outcome", "delay"),
                  idvar = "subject_ID") %>%
  ggplot(aes(delay, HP_choice, color = game1_outcome)) +
  geom_point(position = position_dodge(width = 0.5)) +
  geom_errorbar(aes(ymin = HP_choice - ci, ymax = HP_choice +ci),
                position = position_dodge(width = 0.5), width = 0.3) +
  labs(x = "Delay between two games", 
       y = "Proportion of high-probability choices",
       color = "Game 1 outcome")
```

As noted above, when the high-probability option was the option with a higher expected value, participants predominantly chose the high-probability option with little variation. I therefore plot the data again, this time separately for whether the low-probability or the high-probability was the one with a higher expected value.

```{r}
#| label: fig-game2-HP-choice-separate
#| fig-cap: Proportion of high-probability choices in game 2 as a function of game 1 outcome and delay, separately for when the high-probability option (left) or the low-probability option (right) has a higher expected value. Error bars stand for 95% within-subject confidence intervals.
#| fig-width: 8
#| fig-height: 5

df_HP_choices_separate <- df_exp %>%
  group_by(subject_ID, game1_outcome, delay, high_EV_option) %>%
  summarize(HP_choice = mean(game2_choice == "HP") * 100)

# plot the results
df_HP_choices_separate %>%
  summarySEwithin(measurevar = "HP_choice",
                  withinvars = c("game1_outcome", "delay", "high_EV_option"),
                  idvar = "subject_ID") %>%
  ggplot(aes(delay, HP_choice, color = game1_outcome)) +
  geom_point(position = position_dodge(width = 0.5)) +
  geom_errorbar(aes(ymin = HP_choice - ci, ymax = HP_choice +ci),
                position = position_dodge(width = 0.5), width = 0.3) +
  labs(x = "Delay between two games", 
       y = "Proportion of high-probability choices",
       color = "Game 1 outcome") +
  facet_wrap(~high_EV_option, ncol = 2)
```

Wins and losses seem to have very little influence on choices. However, a potentially interesting observation (if reliable) is that after a delay, participants seem to become more sensitive to expected values in their choices. When the HP option has a higher expected value, they chose the HP option more often after a delay; when the LP option has a higher expected value, they chose the LP option more often after a delay.

### Multilevel analyses

In this analysis, I use the EV ratio between two options, the outcome of game 1 (loss = 0.5, win = -0.5) and whether there was a delay or not (yes = 0.5, no = -0.5) as predictors to predict whether participants chose the HP option or not on each trial. All interactions are also included, and I use the maximum random structure by including both random intercept and all random slopes per participant.

```{r}
# fit a brms model - go for the maximum random structure
fit_HP_choice <- brm(
  game2_choose_HP ~ EV_ratio * game1_outcome_num * delay_num +
    (EV_ratio * game1_outcome_num * delay_num|subject_ID),
  family = bernoulli(link = "logit"),
  prior = 
    c(
      prior(normal(0, 2), class = Intercept),
      prior(normal(0, 1), class = b),
      prior(normal(0, 1), class = sd),
      prior(lkj(2), class = cor)
    ),
  data = df_exp,
  cores = 4,
  iter = 15000,
  warmup = 5000,
  seed = 1234,
  file = "brms-fits/fit_HP_choice",
  backend = "cmdstanr"
)
```

Some general posterior predictive checks. While the model seems to capture the effects of game 1 outcome and delay relatively well, there is a large deviation between the observed and predicted outcome when it comes to EV ratio.

```{r}
#| label: fig-ppc-general
#| fig-cap: General posterior predictive checks.
#| fig-subcap: 
#| - "As a function of delay"
#| - "As a function of game 1 outcome"
#| - "As a function of EV ratio"
#| layout: [[1, 1], [1]]
#| fig-width: 8
#| fig-height: 8


# model diagnostics
# check the trace plot, looks good.
# plot(fit_HP_choice)

# loo(fit_HP_choice)

# posterior predictive checks
yrep <- posterior_predict(fit_HP_choice)

ppc_bars_grouped(df_exp$game2_choose_HP, yrep, 
                 df_exp$delay, prob = 0.95, freq = FALSE)

ppc_bars_grouped(df_exp$game2_choose_HP, yrep, 
                 df_exp$game1_outcome, prob = 0.95, freq = FALSE)

ppc_bars_grouped(df_exp$game2_choose_HP, yrep, 
                 df_exp$EV_ratio, prob = 0.95, freq = FALSE)

# save the raw data frame and yrep as rds files.
# This is needed for the Shiny code below
saveRDS(df_exp, "brms-fits/data.rds")
saveRDS(yrep, "brms-fits/predicted.rds")
```

Make a plot to show the effects of EV ratio and delay.

```{r}
#| label: fig-EV-ratio-delay
#| fig-cap: Probability of choosing the HP option as a function of EV ratio and pause.
#| fig-width: 4
#| fig-height: 4

# Here I will plot the probability of choosing the HP option
# as a function of the EV ratio and whether there was a delay or not
# EV ratio will be the x axis, and delay (yes vs. no) as two colors
# here I define a small adjustment value to the x axis (i.e. EV ratio),
# that moves all no delay condition data points a bit to the left,
# and the delay condition data points to the right, to reduce overlapping.
x_adj_amount <- 0.025

# compute the probability of choosing HP for each participant
# in each condition.
plot_raw_data <- df_exp %>%
  group_by(subject_ID, EV_ratio, delay) %>%
  summarize(p_HP = mean(game2_choose_HP)) %>%
  mutate(
    # add a bit adjustment as mentioned above
    EV_ratio_adj = ifelse(delay == "yes", x_adj_amount, -x_adj_amount),
    EV_ratio_plot = EV_ratio + EV_ratio_adj
  )

# get the predicted prob of choosing HP from brms
int_conds <- list(
  EV_ratio = unique(df_exp$EV_ratio),
  delay_num = c(-0.5, 0.5)
)

plot_prediction <- 
  conditional_effects(
    fit_HP_choice, 
    "EV_ratio:delay_num",
    int_conditions = int_conds)

# turn the predictions into a tibble
plot_pred <- as_tibble(
  plot_prediction$`EV_ratio:delay_num`
  ) %>%
  mutate(
    # similarly, add a little adjustment to the EV ratio
    delay = ifelse(delay_num == 0.5, "yes", "no"),
    EV_ratio_adj = ifelse(delay == "yes", x_adj_amount, -x_adj_amount),
    EV_ratio_plot = EV_ratio + EV_ratio_adj
  )

# plot the results
EV_delay_plot <- plot_raw_data %>%
  ggplot(aes(EV_ratio_plot, p_HP, color = delay)) +
  # plot raw data per participant as transparent open circles
  # jitter the dots vertically a bit to reduce overlapping
  geom_jitter(alpha = 0.2, height = 0.05, shape = 1) +
  # connect the predicted prob of choosing HP with lines
  geom_line(data = plot_pred, 
            aes(x = EV_ratio_plot,
                y = estimate__,
                group = delay),
            alpha = 0.4) +
  # estimates as solid points
  geom_point(data = plot_pred, 
             aes(EV_ratio_plot, 
                 estimate__)) +
  # add 95% CI as error bars
  geom_errorbar(data = plot_pred,
                aes(x = EV_ratio_plot, 
                    y = estimate__,
                    ymin = lower__, 
                    ymax = upper__),
                width = 0.05) +
  labs(x = "EV ratio", 
       y = "P(Choose the HP option)",
       color = "Pause")

EV_delay_plot

# save as a RDS file - makes it easier to combine with other plots
saveRDS(EV_delay_plot, "plots/EV_delay_plot.rds")

# save as a png file
ggsave(filename = "plots/EV_delay_plot.png", 
       EV_delay_plot,
       width = 4, height = 4)
```

### Individual PPC (Shiny)

Next I make a custom posterior predictive check for each participant separately, focusing on the effect of EV ratio. Note that this code uses Shiny, and will only run when the Quarto document is executed within RStudio (click on the Run Document button).

```{r}
#| context: setup
#| include: false

# load libraries
library(tidyverse)
library(wrMisc)
library(bayestestR)
library(shiny)

# load data
observed <- readRDS("brms-fits/data.rds")
predicted <- readRDS("brms-fits/predicted.rds")

# function to compute the median, and boundaries of 95% HDI
compute_summary <- function(x){
  median <- median(x) * 100
  upperCI <- hdi(x)$CI_high * 100
  lowerCI <- hdi(x)$CI_low * 100
  return(c(median = median, upperCI = upperCI, lowerCI = lowerCI))
}
```

```{r}
selectInput(
  inputId = "subject_selected",
  label = "Select a participant number",
  choices = unique(df_exp$subject_ID),
  selected = unique(df_exp$subject_ID)[1],
  multiple = FALSE,
  selectize = TRUE,
  width = NULL,
  size = NULL
)

plotOutput("ppcPlot")
```

```{r}
#| context: server

output$ppcPlot <- renderPlot({
  # select the observed and predicted data for a particular subject
  subject_selected <- input$subject_selected

  df_exp_one_pp <- observed %>% filter(subject_ID == subject_selected)
  yrep_one_pp <- predicted[, observed$subject_ID == subject_selected]

  # compute the mean proportion of choosing the HP option,
  # given each EV ratio level, for each simulation separately
  yrep_prop <- rowGrpMeans(yrep_one_pp, grp = df_exp_one_pp$EV_ratio)

  # turn the data into a tibble
  yrep_prop <- as_tibble(yrep_prop)

  # compute the median and 95% HDI for each EV ratio level
  yrep_summary <- sapply(yrep_prop, compute_summary)

  # do some data formatting
  yrep_summary <- as_tibble(t(yrep_summary), rownames = NA) %>%
    rownames_to_column(var = "EV_ratio") %>%
    mutate(EV_ratio = as.numeric(EV_ratio)) %>%
    arrange(EV_ratio)

  # compute the observed proportion of HP choices for each EV ratio
  y_summary <- df_exp_one_pp %>%
    group_by(EV_ratio) %>%
    summarize(y = mean(game2_choice == "HP") * 100) %>%
    arrange(EV_ratio) %>%
    select(-EV_ratio)

  # combine the observed and simulated data
  summary <- yrep_summary %>%
    bind_cols(y_summary)

  # plot the results
  ggplot(summary, aes(EV_ratio, y)) +
    # the observed proportion of HP choices as gray dots
    geom_point(size = 4, color = "gray") +
    # the median predicted proportion of HP choices as blue horizontal lines
    geom_point(aes(EV_ratio, median), 
               color = "deepskyblue", shape = "-", size = 10) +
    # the 95% HDI of the predicted HP choices as blue vertical lines 
    geom_linerange(aes(ymin = lowerCI, ymax = upperCI), 
                   color = "deepskyblue") +
    ylim(0, 100) +
    labs(x = "EV Ratio", y = "Porportion of HP choices (%)",
         title = paste0("Participant ", subject_selected)) +
    theme_bw()
})
```

```{r}
#| label: fig-effect
#| fig-cap: Posterior distributions of the regression coefficients.
#| fig-width: 10
#| fig-height: 6

# plot the posterior distributions for variables of interest
var_selected <- variables(fit_HP_choice)[2:8]

mcmc_areas(fit_HP_choice, pars = var_selected)
```

```{r}
tab_model(fit_HP_choice)
```

After a delay, participants became more sensitive to the EV ratios in their choices. However, the posterior predictive checks show that the model does not predict the effects of EV ratio very well. When the EV ratio is negative (i.e., when the LP option has a higher EV), the predicted choices show large deviations from the observed choices.

### Robustness check

In the analysis above, I used the somewhat arbitrary cutoff value that participants needed to choose the high-probability option on at least 75% of the catch trials in order to be included in the analysis. Here I repeat the analysis again, each time using a different cutoff value, to check the robustness of the results.

```{r}
# get all unique numbers of catch trials across participants
# on which they picked the high-prob option
high_prob_cutoffs <- high_prob_choices %>%
  filter(trial_type == "catch") %>%
  .$n_HP_choice %>% unique()

# an empty tibble to hold all results
overall_summary <- tibble()

# go through the six cutoff values one by one
# and fit the model with each of the cutoff values
for (high_prob_cutoff in high_prob_cutoffs) {
  # participants to be excluded
  subject_exclude <- high_prob_choices %>%
    filter(trial_type == "catch",
           n_HP_choice < high_prob_cutoff) %>%
    .$subject_ID
  
  df_exp_tmp <- df_main %>%
    filter(!subject_ID %in% subject_exclude,
           # experimental trials only
           trial_type == "exp")
  
  # check the number of participants remaining
  n_subj <-  n_distinct(df_exp_tmp$subject_ID)
  
  # fit a brms model - go for the maximum random structure
  brms_file_name <-
    paste("brms-fits/fit_exp_HP",
          high_prob_cutoff, sep = "_")
  
  fit_HP_choice <- brm(
    game2_choose_HP ~ EV_ratio * game1_outcome_num * delay_num +
      (EV_ratio * game1_outcome_num * delay_num | subject_ID),
    family = bernoulli(link = "logit"),
    prior =
      c(
        prior(normal(0, 2), class = Intercept),
        prior(normal(0, 1), class = b),
        prior(normal(0, 1), class = sd),
        prior(lkj(2), class = cor)
      ),
    data = df_exp_tmp,
    cores = 4,
    iter = 15000,
    warmup = 5000,
    seed = 1234,
    file = brms_file_name,
    backend = "cmdstanr"
  )
  
  # extract summary of the posterior draws
  post_summary <-
    as_tibble(summary(fit_HP_choice)$fixed, rownames = NA) %>%
    rownames_to_column(var = "parameter") %>%
    mutate(n_subj = n_subj,
           HP_cutoff = high_prob_cutoff)
  
  # add it to the overall tibble
  overall_summary <- bind_rows(overall_summary, post_summary)
}
```

```{r}
#| label: fig-robustness
#| fig-cap: Robustness check
#| fig-width: 6
#| fig-height: 8

# do some cleaning and reformatting of the variables
overall_summary_selected <- overall_summary %>%
  # no need to plot the Intercept, not informative
  filter(parameter != "Intercept") %>%
  mutate(
    # Re-code the labels so that it looks better
    param_label = 
      recode(parameter,
             "EV_ratio" = "EV Ratio",
             "game1_outcome_num" = "Previous Outcome (Loss vs. Win)",
             "delay_num" = "Pause (Yes vs. No)",
             "EV_ratio:game1_outcome_num" = "EV Ratio * Previous Outcome",
             "EV_ratio:delay_num" = "EV Ratio * Pause",
             "game1_outcome_num:delay_num" = "Previous Outcome * Pause",
             "EV_ratio:game1_outcome_num:delay_num" = 
               "EV Ratio * Previous Outcome * Pause"),
    param_label = 
      factor(param_label,
             levels = c("EV Ratio",
                        "Previous Outcome (Loss vs. Win)",
                        "Pause (Yes vs. No)",
                        "EV Ratio * Previous Outcome",
                        "EV Ratio * Pause",
                        "Previous Outcome * Pause",
                        "EV Ratio * Previous Outcome * Pause")),
    # rename variables
    CI_upper = `u-95% CI`,
    CI_lower = `l-95% CI`
  )

# data frame for showing the number of participants
df_n_subj <- overall_summary_selected %>%
  filter(parameter == "EV_ratio") %>%
  mutate(HP_cutoff = as.factor(HP_cutoff))

# plot the posterior distributions from all models
robustness_plot <- overall_summary_selected %>%
  mutate(HP_cutoff = as.factor(HP_cutoff)) %>%
  ggplot(aes(HP_cutoff, Estimate)) +
  geom_hline(yintercept = 0, linetype = "dashed") +
  geom_point() +
  geom_errorbar(aes(ymin = CI_lower, ymax = CI_upper),width = 0.2,) +
  # add the number of participants left
  geom_text(data = df_n_subj, aes(HP_cutoff, 0.5, label = n_subj)) +
  facet_wrap(~param_label, ncol = 1, scales = "free_y") +
  labs(x = "Number of high EV choices on catch trials",
       y = "Estimate (log odds-ratio)") 

robustness_plot

ggsave("plots/robustness_Exp1.png", robustness_plot,
       width = 6, height = 8)
```

## Game 2 start RTs

### Data preparation

Next I examine how quickly participants started game 2 after a win vs. after a loss, and also as a function of whether there was a delay or not.

```{r}
# trial number before any exclusion
n_before <- nrow(df_exp)

# exclude trials in which the start RTs were above 5000 milliseconds
df_after <- df_exp %>%
  filter(game2_startRT <= 5000)

# trial number after exclusion, and the proportion of excluded trials
n_after <- nrow(df_after)
exclude_prop <- round((n_before - n_after)/n_before * 100, 2)

# check the number of remaining trials in each cell for each participant
trials_count <- df_after %>%
  count(subject_ID, game1_outcome, delay)
```

Trials in which the start RT of game 2 was above 5000 milliseconds were excluded (`r exclude_prop`% of all trials). For the remaining participants, each cell in the 2 (game 1 outcome, win vs. loss) by 2 (delay, yes vs. no) contains at least `r min(trials_count$n)` trials, which seems reasonable. For each participant, I then compute the mean start RT (for game 2) after a win or a loss in game 1, depending on whether there was a delay or not.

```{r}
#| label: fig-game2-startRT
#| fig-cap: Start RT for game 2 as a function of game 1 outcome and delay. Error bars stand for 95% within-subject confidence intervals.
#| fig-width: 6
#| fig-height: 5

# compute the mean start RT in each cell for each participant
df_startRTs <- df_after %>%
  group_by(subject_ID, game1_outcome, delay) %>%
  summarize(startRT = mean(game2_startRT))

# plot the results
df_startRTs %>%
  summarySEwithin(measurevar = "startRT",
                  withinvars = c("game1_outcome", "delay"),
                  idvar = "subject_ID") %>%
  ggplot(aes(delay, startRT, color = game1_outcome)) +
  geom_point(position = position_dodge(width = 0.5)) +
  geom_errorbar(aes(ymin = startRT - ci, ymax = startRT +ci),
                position = position_dodge(width = 0.5), width = 0.3) +
  labs(x = "Pause between two games", 
       y = "Reaction time of starting game 2 (milliseconds)",
       color = "Game 1 outcome")
```

### Fit model

Fit the data with a hierarchical regression.

```{r}
# since startRT is positively skewed,
# take the natural logarithm of the startRT as the dependent variable
df_startRT <- df_after %>%
  mutate(log_startRT = log(game2_startRT))

# fit a brms model, with student's t as the likelihood function
fit_startRT_student <- brm(
  log_startRT|trunc(ub = log(5000)) ~  
    game1_outcome_num * delay_num +
    (game1_outcome_num * delay_num|subject_ID),
  family = student(),
  # init = list(
  #   list(Intercept = 5.5, nu = 20),
  #   list(Intercept = 6, nu = 20),
  #   list(Intercept = 6.5, nu = 20),
  #   list(Intercept = 7, nu = 20)
  # ),
  prior = 
    c(
      prior(normal(6.5, 1.5), class = Intercept),
      prior(normal(0, 1), class = b),
      prior(normal(0, 1), class = sd),
      prior(normal(0, 1), class = sigma),
      prior(gamma(2, 0.1), class = nu),
      prior(lkj(2), class = cor)
    ),
  data = df_startRT,
  cores = 4,
  iter = 7000,
  warmup = 2000,
  seed = 1234,
  file = "brms-fits/fit_startRT_student",
  backend = "cmdstanr"
  )
```

### Model results

```{r}
# check model diagnostics
# plot(fit_startRT_student)
# the traceplots look okay

# loo(fit_startRT_student)
# All Pareto k estimates are good (k < 0.5).

# posterior predictive check
yrep <- posterior_predict(fit_startRT_student)

# turn the start RT back to the original scale
yrep <- exp(yrep)

ppc_dens_overlay(df_startRT$game2_startRT, yrep[1:50, ])

remove(yrep)
```

```{r, results='asis'}
# have a look at the summary of posterior distributions
tab_model(fit_startRT_student, digits = 2)
```

## Game 2 choice RTs

### Data preparation

Next I examine whether the increased sensitivity towards the EV ratio after a delay may be explained by participants making decisions more slowly in the delay condition.

```{r}
#| label: fig-game2-choiceRT
#| fig-cap: Histogram of all game 2 choice reaction times.
#| fig-width: 5
#| fig-height: 4

# plot a distribution of all game 2 choice RTs
ggplot(df_exp, aes(game2_respRT)) +
  geom_histogram(bins = 40) +
  geom_vline(xintercept = 5000, linetype = "dashed") +
  labs(x = "Game 2 choice reaction time (milliseconds)")
```

To reduce the influence of a few long reaction times, I exclude the trials where the choiceRT was above 5000 milliseconds.

```{r}
#| label: fig-game2-choiceRT-per-cell
#| fig-cap: Choice RT in game 2 as a function of game 1 outcome and delay. Error bars stand for 95% within-subject confidence intervals.
#| fig-width: 6
#| fig-height: 5

# number of trials before exclusion
n_before <- nrow(df_exp)

# exclude trials with choice RTs above 5000 ms
df_choiceRT <- df_exp %>% filter(game2_respRT <= 5000)

# number of trials after exclusion
n_after <- nrow(df_choiceRT)

# proportion of excluded trials
prop_exclude <- round((n_before - n_after)/n_before * 100, 2)

df_choiceRT_summary <- df_choiceRT %>%
  group_by(subject_ID, game1_outcome, delay) %>%
  summarize(choiceRT = mean(game2_respRT))

# plot the choice RTs
df_choiceRT_summary %>%
  summarySEwithin(measurevar = "choiceRT",
                  withinvars = c("game1_outcome", "delay"),
                  idvar = "subject_ID") %>%
  ggplot(aes(delay, choiceRT, color = game1_outcome)) +
  geom_point(position = position_dodge(width = 0.5)) +
  geom_errorbar(aes(ymin = choiceRT - ci, ymax = choiceRT +ci),
                position = position_dodge(width = 0.5), width = 0.3) +
  labs(x = "Delay between two games", 
       y = "Game 2 choice reaction time (milliseconds)",
       color = "Game 1 outcome")
```

### Fit model

I use log-transformed choice RT as the dependent variable (truncated with a upper bound of log(5000), as above). Both game 1 outcome, whether there was a pause or not and their interaction are included as predictors.

```{r}
# log-transform the choice RT
df_choiceRT <- df_choiceRT %>%
  mutate(log_choiceRT = log(game2_respRT))

# fit a brms model, with student's t as the likelihood function
fit_choiceRT_student <- brm(
  log_choiceRT|trunc(ub = log(5000)) ~  
    game1_outcome_num * delay_num +
    (game1_outcome_num * delay_num|subject_ID),
  family = student(),
  prior = 
    c(
      prior(normal(6.5, 1), class = Intercept),
      prior(normal(0, 1), class = b),
      prior(normal(0, 1), class = sd),
      prior(normal(0, 1), class = sigma),
      prior(gamma(2, 0.1), class = nu),
      prior(lkj(2), class = cor)
    ),
  data = df_choiceRT,
  cores = 4,
  iter = 7000,
  warmup = 2000,
  seed = 12345,
  file = "brms-fits/fit_choiceRT_student",
  backend = "cmdstanr",
  refresh = 1
  )
```

### Model results

```{r}
# check model diagnostics
# plot(fit_choiceRT_student)
# the traceplots look okay

# loo(fit_choiceRT_student)
# All Pareto k estimates are good (k < 0.5).

# posterior predictive check
yrep <- posterior_predict(fit_choiceRT_student)

# turn the choice RT back to the original scale
yrep <- exp(yrep)

ppc_dens_overlay(df_choiceRT$game2_respRT, yrep[1:50, ])

remove(yrep)
```

```{r,results='asis'}
tab_model(fit_choiceRT_student, digits = 4)
```

A few findings are worth noting, and largely consistent across the models:

1.  Participants overall chose more quickly after a loss than after a win.

2.  Inserting a pause did not reliably influence how quickly they chose.

3.  Larger EV ratios were associated with faster choices. Participants might find a choice easier to make if the HP option has a higher EV.

4.  Delay interacted with EV ratio, but only in the model assuming a Student's t likelihood. (In the Gaussian model, the effect was in the same direction, but the 95% CI included 0.) Thus, choice RTs were influenced by the EV ratio more after a pause.

The interaction effect are broadly in line with the effect observed above for choices. After a pause, both participants' choices and their choice RTs are more influenced by the EV ratios. Their choices overall do not become slower though after a pause.

## RT plots

```{r}
# compute the mean of log start RT in each cell for each participant
startRT_obs <- df_startRT %>%
  group_by(subject_ID, game1_outcome, delay) %>%
  summarize(obs = mean(log_startRT)) 

# do the same for the log choice RT
choiceRT_obs <- df_choiceRT %>%
  group_by(subject_ID, game1_outcome, delay) %>%
  summarize(obs = mean(log_choiceRT)) 

# get the predicted log start RT from the model
int_conds <- list(
  game1_outcome_num = c(-0.5, 0.5),
  delay_num = c(-0.5, 0.5)
)

startRT_pred <- conditional_effects(
  fit_startRT_student, 
  "game1_outcome_num:delay_num",
  int_conditions = int_conds)

# turn the predictions into a tibble
startRT_pred <- as_tibble(
  startRT_pred$`game1_outcome_num:delay_num`
  ) %>%
  mutate(
    delay = ifelse(delay_num == 0.5, "yes", "no"),
    game1_outcome = ifelse(game1_outcome_num == 0.5, "loss", "win")
  )

# do the same for log choice RT
choiceRT_pred <- conditional_effects(
  fit_choiceRT_student, 
  "game1_outcome_num:delay_num",
  int_conditions = int_conds)

# turn the predictions into a tibble
choiceRT_pred <- as_tibble(
  choiceRT_pred$`game1_outcome_num:delay_num`
  ) %>%
  mutate(
    delay = ifelse(delay_num == 0.5, "yes", "no"),
    game1_outcome = ifelse(game1_outcome_num == 0.5, "loss", "win")
  )

# combine the data frames
startRT_obs <- startRT_obs %>% mutate(var = "Start RT")
startRT_pred <- startRT_pred %>% mutate(var = "Start RT")
choiceRT_obs <- choiceRT_obs %>% mutate(var = "Choice RT")
choiceRT_pred <- choiceRT_pred %>% mutate(var = "Choice RT")

RT_obs <- bind_rows(startRT_obs, choiceRT_obs) %>%
  mutate(var = factor(var, levels = c("Start RT", "Choice RT")))

RT_pred <- bind_rows(startRT_pred, choiceRT_pred) %>%
  mutate(var = factor(var, levels = c("Start RT", "Choice RT")))

# make a plot
RT_plot <- ggplot(RT_obs, aes(game1_outcome, obs, color = delay)) +
  geom_point(position = position_jitterdodge(
    jitter.width = 0.2,
    jitter.height = 0,
    dodge.width = 0.5), alpha = 0.1) +
  geom_point(data = RT_pred, 
             aes(game1_outcome, estimate__,
                 color = delay),
             position = position_dodge(width = 0.5)) +
  geom_errorbar(data = RT_pred,
                aes(x = game1_outcome,
                    y = estimate__,
                    ymin = lower__,
                    ymax = upper__),
                position = position_dodge(width = 0.5),
                width = 0.2) +
  labs(x = "Previous outcome",
       y = "Logarithm of reaction time [log(ms)]",
       color = "Pause") +
  facet_wrap(~var, ncol = 2, scales = "free")

RT_plot

# save as a png file
ggsave("plots/RTs_Exp1.png", RT_plot, width = 4, height = 4)

# also save as a rds file for later use
saveRDS(RT_plot, "plots/RTs_Exp1.rds")
```

## Premature responses

When there is a pause between game 1 and game 2, the program also registers whether participants make any responses during the 4-second pause. First, I examine how frequently people made such premature responses, and the number of premature responses during one pause.

```{r}
#| label: fig-premature
#| fig-cap: Premature responses during the delay between game 1 and game 2.
#| fig-subcap: 
#| - "Proprotion of delay trials on which premature responses occurred"
#| - "Histogram of the number of premature responses during a delay"
#| layout: [[1], [1]]
#| fig-width: 5
#| fig-height: 4

# select trials where there was a pause
df_premature <- df_exp %>%
  filter(delay == "yes")

# plot the proportion of trials on which people made premature responses
df_premature %>%
  group_by(subject_ID) %>%
  summarize(premature_prop = mean(game2_delay_premature > 0) * 100) %>%
  ggplot(aes(subject_ID, premature_prop)) +
  geom_point() +
  labs(x = "Participant ID", 
       y = "Proportion of trials with premature responses")

# plot the number of premature responses when they did occur
df_premature %>%
  filter(game2_delay_premature > 0) %>%
  ggplot(aes(game2_delay_premature)) +
  geom_bar() +
  labs(x = "Number of premature responses")
```

@fig-premature shows that participants overall made premature responses frequently (i.e. on many delay trials), but most of the time they only made one premature response during the 4-second pause. This suggests that they probably pressed the space bar quickly after game 1, in an attempt to start game 2, but stopped responding when they realized that they had to wait. Being so frustrated that people started pressing keys multiple times during the delay, actually did not occur very frequently. As such, I do not conduct further analyses on these premature responses.

## Summary

Wins and losses in game 1 do not influence how quickly people started game 2, but how quickly they chose in game 2 (faster choices after losses compared to wins). Wins and losses do not influence people's choices in game 2. There is an effect of pause though. After a pause, their choices become more sensitive to the EV ratio of the two options.
